{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import create_table\n",
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine\n",
    "# uri = 'mysql+pymysql://root:password@localhost:3308/news_oil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from models import db_connect,create_table\n",
    "from check_pro import return_no_processed_df\n",
    "from utils import wash_process, extract_img_links, read_xlsx, gen_keywords_pair, match_keyword, match_country_region, \\\n",
    "    chopoff, match_company,rematch_keywords,match_topic,match_storage,get_mark_urls,mark_url,add_same_key,\\\n",
    "    remove_intell_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utils.wash_process(x)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wash_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "    table_name = ['news_oil_oe','world_oil']\n",
    "    table_name_pro = ['news_oil_oe_pro','world_oil_pro']\n",
    "    engine = db_connect()\n",
    "    create_table(engine)\n",
    "    cate_file = 'input_data/categories_list.xlsx'\n",
    "    df_dicts = read_xlsx(cate_file)\n",
    "\n",
    "    #==================== generate all the keyword and category pair==================================\n",
    "    # country section\n",
    "    df_dicts['country'].columns = ['region', 'country', 'key_words_chinese', 'key_words_english']  ## rename cols\n",
    "    country_keywords_pair = gen_keywords_pair(df_dicts['country'], 2, [3, 4])\n",
    "    # region section\n",
    "    df_dicts['region'].columns = ['region', 'chinese_keywords', 'english_keywords']\n",
    "    region_df = df_dicts['region']\n",
    "    region_df.columns = ['region', 'chinese_keywords', 'english_keywords']\n",
    "    region_df['chinese_keywords'] = region_df['chinese_keywords'].apply(lambda x: x.split('，')[0])\n",
    "    region_df['english_keywords'] = region_df['english_keywords'].apply(lambda x: x.split('、')[0])\n",
    "    state_keywords_pair = gen_keywords_pair(region_df, 1, [2, 3])\n",
    "    ##genreate the country-region dictionay data for adding the region data from country list\n",
    "    countries = df_dicts['country']['country'].values\n",
    "    regions = df_dicts['country']['region'].values\n",
    "    country_region = {}\n",
    "    for country, region in zip(countries, regions):\n",
    "        country_region[country] = region\n",
    "    ## company section\n",
    "    df_company = df_dicts['company']\n",
    "    df_company.columns = ['country', 'business', 'company', 'keywords']\n",
    "    df_company['keywords'] = df_company['keywords']. \\\n",
    "        apply(lambda x: chopoff(x)). \\\n",
    "        apply(lambda x: x.strip()).apply(lambda x: x.strip().split('、'))\n",
    "    company_keyword_pair = {}\n",
    "    companies = df_company['company'].values\n",
    "    keywords = df_company['keywords'].values\n",
    "    for company, keyword in zip(companies, keywords):\n",
    "        company_keyword_pair[company] = keyword\n",
    "    # print(company_keyword)\n",
    "    company_business = {}\n",
    "    companies = df_company['company'].values\n",
    "    business = df_company['business'].values\n",
    "    for company, business in zip(companies, business):\n",
    "        company_business[company] = business\n",
    "\n",
    "    company_country = {}\n",
    "    companies = df_company['company'].values\n",
    "    counties = df_company['country'].values\n",
    "    for company, country in zip(companies, counties):\n",
    "        company_country[company] = country\n",
    "\n",
    "    ## topic section\n",
    "\n",
    "    df_dicts['subcategory'].columns = ['subset', 'topic', 'chinese_keywords', 'english_keywords']\n",
    "    df_cate = df_dicts['subcategory']\n",
    "    df_cate.iloc[28].topic = '其他能源类型'\n",
    "    df_cate.dropna(inplace=True)\n",
    "    df_cate['english_keywords'] = df_cate['english_keywords'].astype('str').apply(lambda x: x.split('、'))\n",
    "    df_cate['chinese_keywords'] = df_cate['chinese_keywords'].astype('str').apply(lambda x: x.split('、'))\n",
    "\n",
    "    ## rename all the chinese and english keywords\n",
    "    geologies = df_cate.iloc[4].chinese_keywords\n",
    "    smart_geology = df_cate.iloc[16].chinese_keywords\n",
    "    drilling = df_cate.iloc[5].chinese_keywords\n",
    "    smart_drilling = df_cate.iloc[17].chinese_keywords\n",
    "    well_test = df_cate.iloc[6].chinese_keywords\n",
    "    smart_test = df_cate.iloc[18].chinese_keywords\n",
    "    production = df_cate.iloc[7].chinese_keywords\n",
    "    smart_production = df_cate.iloc[19].chinese_keywords\n",
    "    transport = df_cate.iloc[12].chinese_keywords + \\\n",
    "                df_cate.iloc[13].chinese_keywords + \\\n",
    "                df_cate.iloc[14].chinese_keywords + \\\n",
    "                df_cate.iloc[15].chinese_keywords\n",
    "    smart_transport = df_cate.iloc[20].chinese_keywords\n",
    "\n",
    "    geologies_english = df_cate.iloc[4].english_keywords\n",
    "    smart_geology_english = df_cate.iloc[16].english_keywords\n",
    "    drilling_english = df_cate.iloc[5].english_keywords\n",
    "    smart_drilling_english = df_cate.iloc[17].english_keywords\n",
    "    well_test_english = df_cate.iloc[6].english_keywords\n",
    "    smart_test_english = df_cate.iloc[18].english_keywords\n",
    "    production_english = df_cate.iloc[7].english_keywords\n",
    "    smart_production_english = df_cate.iloc[19].english_keywords\n",
    "    transport_english = df_cate.iloc[12].english_keywords + \\\n",
    "                        df_cate.iloc[13].english_keywords + \\\n",
    "                        df_cate.iloc[14].english_keywords + \\\n",
    "                        df_cate.iloc[15].english_keywords\n",
    "    smart_transport_english = df_cate.iloc[20].english_keywords\n",
    "\n",
    "    ##generate mixed keywords\n",
    "    smart_geologies_chinese_mixed = rematch_keywords(geologies, smart_geology)\n",
    "    smart_drill_chinese_mixed = rematch_keywords(drilling, smart_drilling)\n",
    "    smart_well_test_chinese_mixed = rematch_keywords(well_test, smart_test)\n",
    "    smart_production_chinese_mixed = rematch_keywords(production, smart_production)\n",
    "    smart_transport_chinese_mixed = rematch_keywords(transport, smart_transport)\n",
    "    smart_geologies_english_mixed = rematch_keywords(geologies_english, smart_geology_english)\n",
    "    smart_drill_english_mixed = rematch_keywords(drilling_english, smart_drilling_english)\n",
    "    smart_well_test_english_mixed = rematch_keywords(well_test_english, smart_test_english)\n",
    "    smart_production_english_mixed = rematch_keywords(production_english, smart_production_english)\n",
    "    smart_transport_english_mixed = rematch_keywords(transport_english, smart_transport_english)\n",
    "    ## change the keywords with such mixed ones\n",
    "    df_cate.iloc[16].chinese_keywords = smart_geologies_chinese_mixed\n",
    "    df_cate.iloc[17].chinese_keywords = smart_drill_chinese_mixed\n",
    "    df_cate.iloc[18].chinese_keywords = smart_well_test_chinese_mixed\n",
    "    df_cate.iloc[19].chinese_keywords = smart_production_chinese_mixed\n",
    "    df_cate.iloc[20].chinese_keywords = smart_transport_chinese_mixed\n",
    "    df_cate.iloc[16].english_keywords = smart_geologies_english_mixed\n",
    "    df_cate.iloc[17].english_keywords = smart_drill_english_mixed\n",
    "    df_cate.iloc[18].english_keywords = smart_well_test_english_mixed\n",
    "    df_cate.iloc[19].english_keywords = smart_production_english_mixed\n",
    "    df_cate.iloc[20].english_keywords = smart_transport_english_mixed\n",
    "\n",
    "    df_cate['keywords'] = df_cate['chinese_keywords'] + df_cate['english_keywords']\n",
    "    ## preparing the category-keywords pair\n",
    "    topic_keywords = {}\n",
    "    topics = df_cate['topic'].values\n",
    "    keywords = df_cate['keywords'].values\n",
    "    for topic, keyword in zip(topics, keywords):\n",
    "        topic_keywords[topic] = keyword\n",
    "    topic_subcategory = {}\n",
    "\n",
    "    topics = df_cate['topic'].values\n",
    "    subcategory = df_cate['subset'].values\n",
    "    for topic, keyword in zip(topics, subcategory):\n",
    "        topic_subcategory[topic] = keyword\n",
    "    topic_subcategory['石油公司'] = '能源公司'\n",
    "    topic_subcategory['油服公司'] = '能源公司'\n",
    "    ## field section\n",
    "    df_dicts['field'].columns = ['field', 'keyword']\n",
    "    df_field = df_dicts['field']\n",
    "    df_field['merged_keywords'] = df_field['keyword']. \\\n",
    "        apply(lambda x: chopoff(x)). \\\n",
    "        apply(lambda x: x.strip()).apply(lambda x: x.strip().split('、'))\n",
    "    field_keyword = {}\n",
    "    field = df_field['field'].values\n",
    "    keyword = df_field['merged_keywords'].values\n",
    "    for fie, key in zip(field, keyword):\n",
    "        field_keyword[fie] = key\n",
    "    field_keyword['MESSLAH'] = ['MESSLAH', 'MESSLA'] ## some correction of data\n",
    "    ## storage section\n",
    "    df_dicts['storage'].columns = ['country', 'storage', 'keyword']\n",
    "    df_storage = df_dicts['storage']\n",
    "    storage_keyword = {}\n",
    "    storage = df_storage['storage'].values\n",
    "    keyword = df_storage['keyword'].values\n",
    "    for stor, key in zip(storage, keyword):\n",
    "        if re.search('/', key):\n",
    "            storage_keyword[stor] = key.split('/')\n",
    "        else:\n",
    "            storage_keyword[stor] = [key.strip()]\n",
    "    storage_keyword['MOLDOVA  (FALTICENI)'] = 'MOLDOVA'\n",
    "    storage_keyword['CHESHIRE (HOLFORD GS)'] = 'Cheshire'\n",
    "    storage_keyword['HILL TOP FARM  (CHESHIRE EXISTING)'] = 'Hill Top Farm'\n",
    "    storage_keyword['HILL TOP FARM  (CHESHIRE EXPANSION)'] = 'Hill Top Farm'\n",
    "    storage_keyword['KIRK RANCH  (BOBBY BURNS #1)'] = 'KIRK RANCH'\n",
    "    storage_keyword['CLEMENS NE  (FRIO B)'] = 'CLEMENS,N.E.'\n",
    "    ## get country according to the storage\n",
    "    storage_country = {}\n",
    "    storage = df_storage['storage'].values\n",
    "    country = df_storage['country'].values\n",
    "    for stor, coun in zip(storage, country):\n",
    "        storage_country[stor] = coun\n",
    "\n",
    "    mark_urls = get_mark_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_country  #dictionary of one-one\n",
    "# country_region\n",
    "# company_business\n",
    "# topic_subcategory\n",
    "# company_country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_keywords_pair\n",
    "# state_keywords_pair\n",
    "# company_keyword_pair\n",
    "# topic_keywords\n",
    "# field_keyword\n",
    "# storage_keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b1846f22adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_df' is not defined"
     ]
    }
   ],
   "source": [
    "raw_df['new_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ==================== reach the process section for each category==================================\n",
    "    df= pd.DataFrame()\n",
    "    for table_pair in zip(table_name, table_name_pro):\n",
    "        pre_data = return_no_processed_df(table_pair[0], table_pair[1], engine)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = return_no_processed_df('cnpc_news', 'cnpc_news_pro', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"sj-main\">\\n        \\n          <div class=\"as04\" style=\" display:block; width:650px;\">\\n\\n          \\t <p style=\"text-align:left\"><strong>\\u3000\\u3000中国石油网消息</strong>（特约记者李冬铃\\xa0通讯员于春晶）抚顺石化石油二厂组织干部员工开动脑筋集思广益，用智慧实现持续挖潜创效。7月31日统计数据显示，上半年，石油二厂制氢车间员工通过优化、改造水系统，创效达490余万元。</p>\\r\\n\\r\\n<p style=\"text-align:left\">\\u3000\\u3000制氢车间在生产过程中，一方面要利用脱盐水生产蒸汽，另一方面装置产生的凝结水还要排放掉，如何让两者合二为一用凝结水产生蒸汽，成为这个车间做活“水文章”的主要攻关方向。这个厂强化自产凝结水管理，解决了自产凝结水电导率超标的问题，然后将其回收到除氧器代替新鲜脱盐水生产蒸汽。项目实施后，每小时回收凝结水36吨、节省除盐水36吨，降低了生产成本。</p>\\r\\n\\r\\n<p style=\"text-align:left\">\\u3000\\u3000不仅如此，这个车间又将循环利用的着眼点放在排水上。技术人员对自产排水进行水质分析后发现，各项指标优于循环水场的补水指标，对工艺流程进行改造后，将排水接入循环水管线，用作循环水场补水使用，实施后每小时可节约循环水4吨。在此基础上，这个车间合理整定配汽控制阀PID参数，实现了制氢装置自产中压蒸汽稳定并网，解决了低压蒸汽过剩、中压蒸汽不足的问题，每月为企业节省外购中压蒸汽费用。</p>\\r\\n\\r\\n<p style=\"text-align:left\">\\u3000\\u3000制氢装置转化炉是原料与水蒸气发生转化反应的关键场所，炉膛负压的高低直接关系到装置安全生产、能耗和转化反应完成情况。这个车间严格控制转化炉氧含量及排烟温度，及时调整逆放调节阀开度，适时调整PIC-4005压力控制值，控制解吸气流量相对平稳。通过这一系列优化措施，避免了炉膛负压波动引起装置联锁停车的风险，同时提高了氢气收率，转化炉热效率也提升至92.8%，为企业节能创效增添了新的增长点。</p>\\r\\n \\r\\n\\r\\n\\r\\n\\r\\n          \\t\\n          \\t</div>\\n        </div>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wash_hart_energy_process(x,class_name):\n",
    "    '''\n",
    "    '''\n",
    "    contents = []\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    ancestor = soup.find('div',attrs={'class':class_name})\n",
    "#     print(an)\n",
    "    for child in [child for child in ancestor.children if not isinstance(child,NavigableString)][:2]:\n",
    "        for desc in child.descendants:\n",
    "            if desc.name == 'img' and desc.has_attr('src'):\n",
    "                contents.append(desc)\n",
    "            if desc.name == 'p' and not desc.has_attr('class'):\n",
    "                contents.append(desc.text.replace(u'\\xa0', u''))\n",
    "            if desc.name == 'div' and desc.has_attr('class') and desc.attrs['class']==\"userAlready\":\n",
    "                break\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_oil(x,attrs):\n",
    "    '''grab \n",
    "    '''\n",
    "    contents = []\n",
    "    chop_index = None\n",
    "    soup = BeautifulSoup(test, 'lxml')\n",
    "    ancestor = soup.find('div',attrs={'id':'news'})\n",
    "    # print(list(ancestor.children))\n",
    "    for child in [child for child in ancestor.children if not isinstance(child,NavigableString)]:\n",
    "    #     print(child)\n",
    "        if child.name=='p'and not child.has_attr('class')  :\n",
    "            contents.append(child.text.replace(u'\\xa0', u''))\n",
    "    #     elif child.name=='p'and child.find('strong') and not child.find('strong'):\n",
    "    #         break\n",
    "        elif child.name=='div':\n",
    "            for desc in child.descendants:\n",
    "                if not isinstance(desc,NavigableString):\n",
    "                    if desc.name == 'img' and desc.has_attr('src') and re.search('/media',desc.attrs['src']):\n",
    "                        contents.append(desc)\n",
    "        elif child.name=='h2' and re.search(r'Related News',child.string):\n",
    "            break\n",
    "\n",
    "    if contents.index('REFERENCES'):\n",
    "        chop_index = contents.index('REFERENCES')\n",
    "    contents = contents[:chop_index]\n",
    "        \n",
    "    return contents\n",
    "\n",
    "    \n",
    "\n",
    "#     else:\n",
    "#         continue\n",
    "#         if desc.name == 'p' and not desc.has_attr('class'):\n",
    "#             contents.append(desc.text.replace(u'\\xa0', u''))\n",
    "#         if desc.name =='p' and desc.find('strong'):\n",
    "#             break\n",
    "#         if desc.name == 'p' and desc.has('class') and desc.attrs['class']==\"userAlready\":\n",
    "#             break\n",
    "\n",
    "# return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = world_oil(test,attrs={'id':'news'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = req.get('https://www.worldoil.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_world_oil_hot():\n",
    "    '''get front page url'''\n",
    "    ele_urls = []\n",
    "    host='https://www.worldoil.com'\n",
    "    res = req.get(host)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    cols = soup.find_all('div',attrs={'class':'col-sm-6'})[:2]\n",
    "    for col in cols:\n",
    "    #     print(col)\n",
    "        urls = col.find_all('a')\n",
    "        for url in urls:\n",
    "            ele_urls.append(host+url['href'])\n",
    "    return ele_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hart_energy_hot():\n",
    "    \n",
    "    ele_urls = []\n",
    "    host='https://www.hartenergy.com'\n",
    "    res = req.get(host)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    latest = soup.find('div',attrs={'id':'homepage-latest'})\n",
    "    rows  = latest.find_all('a')\n",
    "    for row in rows[:-1]:\n",
    "        ele_urls.append(host+row['href'])\n",
    "    return ele_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_cnpc_hot():\n",
    "    '''compare with the tiltes'''\n",
    "    titles=[]\n",
    "    host='http://news.cnpc.com.cn/toutiao/'\n",
    "    res = req.get(host)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    lists = soup.find('div',attrs={'class':'list18'})\n",
    "    for row in lists.find_all('li',attrs={'class':'ejli'}):\n",
    "        title = row.find('a').text.strip()\n",
    "        titles.append(title)\n",
    "        \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wash_process(x,class_name):\n",
    "    '''\n",
    "    '''\n",
    "    contents = []\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    ancestor = soup.find('div',attrs={'class':class_name})\n",
    "    for desc in ancestor.descendants:\n",
    "        if desc.name == 'img' and desc.has_attr('src'):\n",
    "            contents.append(desc)\n",
    "        if desc.name == 'p' and not desc.has_attr('class'):\n",
    "            contents.append(desc.text.replace(u'\\xa0', u''))\n",
    "        if desc.name == 'div' and desc.has_attr('class') and desc.attrs['class']==\"userAlready\":\n",
    "            break\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = pd.read_sql_table('hart_energy',engine,columns=['id','title','content','url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content = pre_data['content'].apply(lambda x:wash_hart_energy_process(x,'article-content-wrapper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-183da3c1cdd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ==================== reach the process section for each category==================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtable_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name_pro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpre_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_no_processed_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_name' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "        print(pre_data.head())\n",
    "#         print(pre_data.head(),pre_data.info(),type(pre_data))\n",
    "        if len(pre_data) == 0:  ## no dataframe needed to be processed\n",
    "            break\n",
    "        else:\n",
    "            raw_df = pre_data.iloc[0:1]\n",
    "            if re.search(r'_oe',table_pair[0]): ## oedigital form of data\n",
    "                raw_df['format_pub_time'] = raw_df['pub_time']\\\n",
    "                .apply(lambda x: datetime.strptime(x, \"%B %d, %Y\").strftime('%Y/%m/%d'))\\\n",
    "                .apply(lambda x:datetime.strptime(x,\"%Y/%m/%d\"))\n",
    "                  ##make the dataframe name consistent\n",
    "                # print(raw_df['url'][0],raw_df['content'],raw_df['content'][0],type(raw_df['content'][0]))\n",
    "                # break\n",
    "            raw_df['new_content'] = raw_df['content'].apply(lambda x: wash_process(x, div_class_name))\n",
    "            raw_df['img_urls_new'] = raw_df['new_content'].apply(lambda x: extract_img_links(x))\n",
    "            # raw_df['new_content'] =\n",
    "\n",
    "            raw_df['format_crawl_time'] = raw_df['crawl_time'].apply(lambda x: x.strip()[:10]) \\\n",
    "                .apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\").strftime('%Y/%m/%d')) \\\n",
    "                .apply(lambda x:datetime.strptime(x,\"%Y/%m/%d\"))\n",
    "#             print(raw_df.head())\n",
    "            \n",
    "\n",
    "            df = raw_df[['id', 'author', 'categories', 'preview_img_link',\n",
    "                             'title', 'url', 'new_content', 'img_urls_new',\n",
    "                             'format_pub_time', 'format_crawl_time']]\n",
    "\n",
    "            ## create first checkpoint\n",
    "            ## country keyword section\n",
    "            df['country_keyword'] = df['new_content'].astype('str'). \\\n",
    "                apply(lambda x: match_keyword(x, country_keywords_pair))\n",
    "            ## region keyword sections\n",
    "            ## perform the matching according to the region keywords\n",
    "            df['region_keywords'] = df['new_content'].astype('str') \\\n",
    "                .apply(lambda x: match_keyword(x, state_keywords_pair))\n",
    "\n",
    "            ## determin the region according to the country keyword\n",
    "            df['regions_country'] = df['country_keyword'] \\\n",
    "                .apply(lambda x: match_country_region(x, country_region))\n",
    "            print('reach to process company section')\n",
    "            ## company sections\n",
    "            df['company_keyword'] = df['new_content'].astype('str') \\\n",
    "                .apply(lambda x: match_company(x, company_keyword_pair))\n",
    "            df['business_company'] = df['company_keyword']. \\\n",
    "                apply(lambda x: match_country_region(x, company_business))\n",
    "            df['country_matched_by_company'] = df['company_keyword']. \\\n",
    "                apply(lambda x: match_country_region(x, company_country))\n",
    "\n",
    "            ## topic section\n",
    "            df['topic_keyword'] = df['new_content'].astype('str').apply(lambda x: match_topic(x, topic_keywords))\n",
    "            df['topic_keyword'] = df['business_company'] + df['topic_keyword']\n",
    "            df['subcategory_by_topic'] = df['topic_keyword']. \\\n",
    "                apply(lambda x: match_country_region(x, topic_subcategory))\n",
    "\n",
    "            ##field section\n",
    "            df['field_keyword'] = df['new_content'].astype('str') \\\n",
    "                .apply(lambda x: match_company(x, field_keyword))\n",
    "\n",
    "            ## storage section\n",
    "            df['storage_keyword'] = df['new_content'].astype('str') \\\n",
    "                .apply(lambda x: match_storage(x, storage_keyword))\n",
    "            df['country_storage'] = df['storage_keyword'] \\\n",
    "                .apply(lambda x: match_country_region(x, storage_country))\n",
    "            ## mark or not\n",
    "            df['mark_note_by_url'] = df['url'].apply(lambda x: mark_url(x, mark_urls))\n",
    "\n",
    "            print('reach to post process of data')\n",
    "            ##post process\n",
    "            df['regions'] = df['region_keywords'] + df['regions_country']\n",
    "            df['country'] = df['country_keyword']\n",
    "            df['company_merged'] = df['company_keyword'].apply(lambda x: add_same_key(x))\n",
    "            df['regions_merged'] = df['regions'].apply(lambda x: add_same_key(x))\n",
    "            df['country_merged'] = df['country'].apply(lambda x: add_same_key(x))\n",
    "            df['topic_merged'] = df['topic_keyword'].apply(lambda x: add_same_key(x))\n",
    "            df['subcategory_merged'] = df['subcategory_by_topic'].apply(lambda x: add_same_key(x))\n",
    "            df['country_matched_by_company_merged'] = df['country_matched_by_company'].apply(lambda x: add_same_key(x))\n",
    "\n",
    "            df['new_content'] = raw_df['new_content'] \\\n",
    "                .apply(lambda x: '\\n'.join([str(ele).strip() for ele in x]))\n",
    "\n",
    "            df['topic_merged'] = df['topic_merged'].astype('str').apply(lambda x: remove_intell_topic(x))\n",
    "            df['topic_merged'] = df['topic_merged'].astype('str')\n",
    "            spend_time = round(time.time() -start_time,1)\n",
    "#             print('spend time',spend_time,' to process data',df.info())\n",
    "            df['source'] = 'www.oedigital.com'\n",
    "            df['abstracts'] = df['title']\n",
    "\n",
    "            result = df[['source', 'title', 'abstracts', 'preview_img_link', 'url', 'format_pub_time',\n",
    "                         'author', 'new_content', 'categories',\n",
    "                         'img_urls_new', 'format_crawl_time', 'regions_merged',\n",
    "                         'country_merged', 'company_keyword', 'country_matched_by_company_merged',\n",
    "                         'subcategory_merged', 'topic_merged', 'field_keyword', 'storage_keyword', 'mark_note_by_url'\n",
    "                         ]]\n",
    "            result['orig_id']=df['id']\n",
    "#             result.index =df.index\n",
    "#             print('the column name of result is',result.columns)\n",
    "            # print(result.head(),result.columns,result.info(),result[0:1].values)\n",
    "            result['preview_img_link'] = result['preview_img_link'].astype('str')\n",
    "            result['img_urls_new'] = result['img_urls_new'].astype('str')\n",
    "            result['regions_merged'] = result['regions_merged'].astype('str')\n",
    "            result['country_merged'] = result['country_merged'].astype('str')\n",
    "            result['company_keyword'] = result['company_keyword'].astype('str')\n",
    "            result['country_matched_by_company_merged'] = result['country_matched_by_company_merged'].astype('str')\n",
    "            result['subcategory_merged'] = result['subcategory_merged'].astype('str')\n",
    "            result['field_keyword'] = result['field_keyword'].astype('str')\n",
    "            result['storage_keyword'] = result['storage_keyword'].astype('str')\n",
    "            result['mark_note_by_url'] = result['mark_note_by_url'].astype('str')\n",
    "            # test = result[0:1].values\n",
    "#             print(result.info())\n",
    "#             result.to_csv('oe_backup.csv')\n",
    "            # print(result.iloc[0:1].values)\n",
    "#             print(result.columns)\n",
    "#             result_post =pd.read_csv('oe_backup.csv')\n",
    "\n",
    "\n",
    "            result.to_sql(table_pair[1],engine,if_exists='append',index=False)\n",
    "#             result_post.to_sql('test_table_1',engine,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_abstracts = df['new_content'][0][:340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstracts'] = df['new_content'].apply(lambda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_abstracts = df['new_content'][0]\n",
    "def gen_abstracts(x):\n",
    "    '''generate new abstracts'''\n",
    "    span_length = 0\n",
    "    if re.search('<img .+>',x):\n",
    "        img_tags = re.findall('<img .+>',x)\n",
    "        for img_tag in img_tags:\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tags = re.finditer('<img .+>',df['new_content'][0])\n",
    "\n",
    "tags = [img_tag for img_tag in img_tags]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 309)\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "taken_length = 0\n",
    "## if length>340:\n",
    "for i in range(len(tags)):\n",
    "    take_by_img = tag.span()\n",
    "    if take_by_img[0]<340:\n",
    "        print(take_by_img)\n",
    "        img_tag_length = take_by_img[1] - take_by_img[0]\n",
    "        print(img_tag_length)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['new_content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karen Boman speaks with Prasantha Jayakody of Bsquare about the benefits of condition-based maintenance and the challenges of implementation in the oil and gas industry.\\n\\nGraphic from Bsquare.\\nReducing non-productive time related to equipment maintenance and breakdowns is necessary during the downturn as companies seek cost reductions. To'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<img .+>','',df['new_content'][0])[:340]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(6786, 6924), match='<img alt=\"\" src=\"images/items/2017-08/auto_karen/>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title  \\\n",
      "0   1                              Improving maintenance   \n",
      "1   2  Eidesvik Equips Fleet with Energy Efficiency S...   \n",
      "2   3  Caterpillar to Buy Weir's Oil and Gas Business...   \n",
      "3   4  PHOTO: Heerema's Heavy Lifter Installs Yaxche-...   \n",
      "4   5  Olympic, Safeway Team Up for Offshore Wind Far...   \n",
      "\n",
      "                                           pre_title                 author  \\\n",
      "0                              Improving maintenance            Karen Boman   \n",
      "1  Eidesvik Equips Fleet with Energy Efficiency S...                   None   \n",
      "2  Caterpillar to Buy Weir's Oil and Gas Business...  Aakash Jagadeesh Babu   \n",
      "3  PHOTO: Heerema's Heavy Lifter Installs Yaxche-...               OE Staff   \n",
      "4  Olympic, Safeway Team Up for Offshore Wind Far...                   None   \n",
      "\n",
      "            pub_time                                   preview_img_link  \\\n",
      "0     August 1, 2017                                               None   \n",
      "1      June 16, 2020  https://images.oedigital.com/images/maritime/w...   \n",
      "2    October 5, 2020  https://images.oedigital.com/images/maritime/w...   \n",
      "3    August 21, 2020  https://images.oedigital.com/images/maritime/w...   \n",
      "4  November 24, 2020  https://images.oedigital.com/images/maritime/w...   \n",
      "\n",
      "                                             content  \\\n",
      "0  <div class=\"article\">\\r\\n    <h1 itemprop=\"nam...   \n",
      "1  <div class=\"article\">\\r\\n    <h1 itemprop=\"nam...   \n",
      "2  <div class=\"article\">\\r\\n    <h1 itemprop=\"nam...   \n",
      "3  <div class=\"article\">\\r\\n    <h1 itemprop=\"nam...   \n",
      "4  <div class=\"article\">\\r\\n    <h1 itemprop=\"nam...   \n",
      "\n",
      "                                          categories        crawl_time  \\\n",
      "0                                        Maintenance  12/03/2020 20:01   \n",
      "1  ['Technology', 'Offshore', 'Energy', 'Vessels'...  12/03/2020 20:14   \n",
      "2  ['Equipment', 'Technology', 'Energy', 'Mergers...  12/03/2020 20:14   \n",
      "3  ['Energy', 'Industry News', 'Activity', 'South...  12/03/2020 20:14   \n",
      "4  ['Vessels', 'Offshore Wind', 'Europe', 'German...  12/03/2020 20:14   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.oedigital.com/news/446021-improvin...  \n",
      "1  https://www.oedigital.com/news/479368-eidesvik...  \n",
      "2  https://www.oedigital.com/news/482173-caterpil...  \n",
      "3  https://www.oedigital.com/news/481122-photo-he...  \n",
      "4  https://www.oedigital.com/news/483458-olympic-...  \n",
      "    orig_id             source  \\\n",
      "id                               \n",
      "1         1  www.oedigital.com   \n",
      "2       101  www.oedigital.com   \n",
      "3       201  www.oedigital.com   \n",
      "4       301  www.oedigital.com   \n",
      "5       401  www.oedigital.com   \n",
      "\n",
      "                                                title  \\\n",
      "id                                                      \n",
      "1                               Improving maintenance   \n",
      "2                                 Stepping on the gas   \n",
      "3   Brent Crude Tops $40 as Biden's Win Buoys Risk...   \n",
      "4       Ashtead recognized for safety, quality in GoM   \n",
      "5   Colloquy: Offshore helicopters (commuting on a...   \n",
      "\n",
      "                                            abstracts  \\\n",
      "id                                                      \n",
      "1                               Improving maintenance   \n",
      "2                                 Stepping on the gas   \n",
      "3   Brent Crude Tops $40 as Biden's Win Buoys Risk...   \n",
      "4       Ashtead recognized for safety, quality in GoM   \n",
      "5   Colloquy: Offshore helicopters (commuting on a...   \n",
      "\n",
      "                                     preview_img_link  \\\n",
      "id                                                      \n",
      "1                                                None   \n",
      "2                                                None   \n",
      "3   https://images.oedigital.com/images/maritime/w...   \n",
      "4                                                None   \n",
      "5                                                None   \n",
      "\n",
      "                                                  url format_pub_time  \\\n",
      "id                                                                      \n",
      "1   https://www.oedigital.com/news/446021-improvin...      2017-08-01   \n",
      "2   https://www.oedigital.com/news/460034-stepping...      2010-08-09   \n",
      "3   https://www.oedigital.com/news/483037-brent-cr...      2020-11-09   \n",
      "4   https://www.oedigital.com/news/451398-ashtead-...      2015-09-17   \n",
      "5   https://www.oedigital.com/news/454245-colloquy...      2014-11-03   \n",
      "\n",
      "           author                                        new_content  \\\n",
      "id                                                                     \n",
      "1     Karen Boman  Karen Boman speaks with Prasantha Jayakody of ...   \n",
      "2   Meg Chesshyre  Gas is part of the solution to the global ener...   \n",
      "3    Florence Tan  <img alt=\"Joe Biden / Credit: Gage Skidmore - ...   \n",
      "4        OE Staff  Ashtead Technology’s Houston office has secure...   \n",
      "5       Nina Rach  \\n<img alt=\"\" src=\"https://www.oedigital.com/i...   \n",
      "\n",
      "                                           categories  ... format_crawl_time  \\\n",
      "id                                                     ...                     \n",
      "1                                         Maintenance  ...        2020-12-03   \n",
      "2                  ['Engineering', 'Europe', 'Shale']  ...        2020-12-03   \n",
      "3   ['Energy', 'Activity', 'North America', 'Oil P...  ...        2020-12-03   \n",
      "4   ['Activity', 'North America', 'Gulf of Mexico'...  ...        2020-12-03   \n",
      "5   ['Vessels', 'Asia', 'Rigs', 'Africa', 'Safety ...  ...        2020-12-03   \n",
      "\n",
      "                                       regions_merged  \\\n",
      "id                                                      \n",
      "1                                                  []   \n",
      "2                                         [('欧洲', 6)]   \n",
      "3        [('亚洲', 2), ('拉美', 1), ('中东', 2), ('北美', 3)]   \n",
      "4                  [('亚洲', 1), ('北美', 1), ('大洋洲', 1)]   \n",
      "5   [('非洲', 1), ('拉美', 2), ('亚洲', 5), ('独联体', 6), ...   \n",
      "\n",
      "                                       country_merged  \\\n",
      "id                                                      \n",
      "1                                                  []   \n",
      "2                              [('德国', 1), ('挪威', 4)]   \n",
      "3      [('中国', 2), ('委内瑞拉', 1), ('伊朗', 2), ('美国', 3)]   \n",
      "4               [('新加坡', 1), ('墨西哥', 1), ('澳大利亚', 1)]   \n",
      "5   [('俄罗斯', 6), ('中国', 2), ('印度', 2), ('墨西哥', 1),...   \n",
      "\n",
      "                            company_keyword country_matched_by_company_merged  \\\n",
      "id                                                                              \n",
      "1                                        []                                []   \n",
      "2   [('Equinor', 4), ('Aker Solutions', 2)]                       [('挪威', 6)]   \n",
      "3                                        []                                []   \n",
      "4                                        []                                []   \n",
      "5                     [('Marathon Oil', 1)]                       [('美国', 1)]   \n",
      "\n",
      "                                   subcategory_merged  \\\n",
      "id                                                      \n",
      "1                                      [('勘探开发', 16)]   \n",
      "2   [('能源公司', 6), ('行业市场', 4), ('勘探开发', 14), ('油气储...   \n",
      "3                          [('行业市场', 3), ('勘探开发', 3)]   \n",
      "4                                       [('勘探开发', 6)]   \n",
      "5   [('能源公司', 1), ('行业市场', 1), ('勘探开发', 12), ('油气储...   \n",
      "\n",
      "                                         topic_merged  \\\n",
      "id                                                      \n",
      "1   [('钻完井', 2), ('油气生产', 2), ('海上油气', 11), ('HSE'...   \n",
      "2   [('石油公司', 4), ('油服公司', 2), ('资产交易', 3), ('招投标'...   \n",
      "3   [('行业趋势', 2), ('政策法规', 1), ('钻完井', 1), ('油气生产'...   \n",
      "4                           [('海上油气', 2), ('HSE', 4)]   \n",
      "5   [('石油公司', 1), ('资产交易', 1), ('油气生产', 1), ('海上油气...   \n",
      "\n",
      "                               field_keyword storage_keyword mark_note_by_url  \n",
      "id                                                                             \n",
      "1                    [('AUGILA-NAFOORA', 8)]              []            False  \n",
      "2   [('AUGILA-NAFOORA', 13), ('POMPANO', 1)]              []            False  \n",
      "3     [('AUGILA-NAFOORA', 4), ('POPEYE', 1)]              []            False  \n",
      "4                    [('AUGILA-NAFOORA', 1)]              []            False  \n",
      "5                    [('AUGILA-NAFOORA', 6)]              []            False  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Index(['id', 'title', 'pre_title', 'author', 'pub_time', 'preview_img_link',\n",
      "       'content', 'categories', 'crawl_time', 'url'],\n",
      "      dtype='object')\n",
      "Index(['orig_id', 'source', 'title', 'abstracts', 'preview_img_link', 'url',\n",
      "       'format_pub_time', 'author', 'new_content', 'categories',\n",
      "       'img_urls_new', 'format_crawl_time', 'regions_merged', 'country_merged',\n",
      "       'company_keyword', 'country_matched_by_company_merged',\n",
      "       'subcategory_merged', 'topic_merged', 'field_keyword',\n",
      "       'storage_keyword', 'mark_note_by_url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pre_data = return_no_processed_df(table_pair[0], table_pair[1], engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_no_processed_df(table_name, pro_table_name, engine):\n",
    "    '''\n",
    "    compare with two table extract such row has not processed\n",
    "    '''\n",
    "    ori_df = pd.read_sql_table(table_name, engine)\n",
    "    pro_df = pd.read_sql_table(pro_table_name, engine, index_col='id')\n",
    "    \n",
    "#     print(orid_df.head(),pro_df.head())\n",
    "    ## column id has been processed\n",
    "    id_list = pro_df.orig_id.values.tolist()\n",
    "    print(id_list,len(id_list))\n",
    "\n",
    "    # # if len(ori_df) == len(pro_df)\n",
    "    # if len(id_list) == 0:\n",
    "    #     return ori_df\n",
    "    # else:\n",
    "#     print(id_list)\n",
    "    return ori_df[~(ori_df['id'].isin(id_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 101, 201, 301, 401, 501, 601, 701, 801, 901, 1001, 1101, 1201, 1301, 1401, 1501, 1601, 1701, 1801, 1901, 2001, 2101, 2201, 2301, 2401, 2501, 2601, 2701, 2801, 2901, 3001, 3101, 3201, 3301, 3401, 3501, 3601, 3701, 3801, 3901, 4001, 4101, 4201, 4301, 4401, 4501, 4601, 4701, 4801, 4901, 5001, 5101, 5201, 5301, 5401, 5501, 5601, 5701, 5801, 5901, 6001, 6101, 6201, 6301, 6401, 6501, 6601, 6701, 6801, 6901, 7001, 7101, 7201, 7301, 7401, 7501, 7601, 7701, 7801, 7901, 8001, 8101, 8201, 8301, 8401, 8501, 8601, 8701, 8801, 8901, 9001, 9101, 9201, 9301, 9401, 9501, 9601, 9701, 9801, 9901, 10001, 10101, 10201, 10301, 10401, 10501, 10601, 10701, 10801, 10901, 11001, 11101, 11201, 11301, 11401, 11501, 11601, 11701, 11801, 11901, 12001, 12101, 12201, 12301, 12401, 12501, 12601, 12701, 12801, 12901, 13001, 13101, 13201, 13301, 13401, 13501, 13601, 13701, 13801, 13901, 14001, 14101, 14201, 14301, 14401, 14501, 14601, 14701, 14801, 14901, 15001, 15101, 15201, 15301, 15401, 15501, 15601, 15701, 15801, 15901, 16001, 16101, 16201, 16301, 16401, 16501, 16601, 16701, 16801, 16901, 17001, 17101, 17201, 17301, 17401, 17501, 17601, 17701, 17801, 17901, 18001, 18101, 18201, 18301, 18401, 18501, 18601, 18701, 18801, 18901, 19001, 19101, 19201, 19301, 19401, 19501, 19601, 19701, 19801, 19901, 20001, 20101, 20201, 20301, 20401, 20501, 20601, 20701, 20801, 20901, 21001, 21101, 21201, 21301, 21401, 21501, 21601, 21701, 21801, 21901, 22001] 221\n"
     ]
    }
   ],
   "source": [
    "pre_data = return_no_processed_df(table_pair[0], table_pair[1], engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
